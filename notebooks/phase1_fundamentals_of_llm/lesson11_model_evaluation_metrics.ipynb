{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959acf15",
   "metadata": {},
   "source": [
    "# Lesson 11: Methods and Metrics for Model Evaluation\n",
    "\n",
    "## Introduction (2 minutes)\n",
    "\n",
    "Welcome to our lesson on Methods and Metrics for Model Evaluation. In this 30-minute session, we'll explore various techniques for assessing the performance of language models across different tasks.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand key evaluation metrics for different NLP tasks\n",
    "2. Learn how to implement these metrics\n",
    "3. Recognize the importance of task-specific evaluation\n",
    "4. Understand the process of model selection for different use cases\n",
    "\n",
    "## 1. Evaluation Metrics for Various Tasks (20 minutes)\n",
    "\n",
    "### 1.1 Classification Tasks (7 minutes)\n",
    "\n",
    "For tasks like sentiment analysis, spam detection, etc.\n",
    "\n",
    "Key Metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "Example implementation using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67faa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 1, 1]\n",
    "evaluate_classification(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6b672",
   "metadata": {},
   "source": [
    "### 1.2 Generation Tasks (7 minutes)\n",
    "\n",
    "For tasks like machine translation, text summarization, etc.\n",
    "\n",
    "Key Metrics:\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "Example implementation of BLEU score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return sentence_bleu([reference.split()], candidate.split())\n",
    "\n",
    "# Example usage\n",
    "reference = \"The cat is on the mat\"\n",
    "candidate = \"There is a cat on the mat\"\n",
    "bleu_score = calculate_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5d278",
   "metadata": {},
   "source": [
    "### 1.3 Language Model Specific Metrics (6 minutes)\n",
    "\n",
    "Metrics specific to language modeling tasks:\n",
    "\n",
    "- Perplexity\n",
    "- Bits per Character (BPC)\n",
    "\n",
    "Example calculation of perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_perplexity(probabilities):\n",
    "    return np.exp(-np.mean(np.log(probabilities)))\n",
    "\n",
    "# Example usage\n",
    "probabilities = [0.1, 0.2, 0.05, 0.6, 0.05]\n",
    "perplexity = calculate_perplexity(probabilities)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee829fc5",
   "metadata": {},
   "source": [
    "## 2. Model Selection for Different Use Cases (6 minutes)\n",
    "\n",
    "When selecting a model for a specific use case, consider:\n",
    "\n",
    "1. Task requirements (e.g., classification vs. generation)\n",
    "2. Performance on relevant metrics\n",
    "3. Computational resources available\n",
    "4. Inference time requirements\n",
    "5. Fine-tuning potential\n",
    "\n",
    "Example decision process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(task, performance_threshold, max_size, max_inference_time):\n",
    "    models = {\n",
    "        \"ModelA\": {\"performance\": 0.85, \"size\": 500, \"inference_time\": 100},\n",
    "        \"ModelB\": {\"performance\": 0.90, \"size\": 1000, \"inference_time\": 200},\n",
    "        \"ModelC\": {\"performance\": 0.95, \"size\": 2000, \"inference_time\": 300}\n",
    "    }\n",
    "    \n",
    "    selected_model = None\n",
    "    for model, specs in models.items():\n",
    "        if (specs[\"performance\"] >= performance_threshold and\n",
    "            specs[\"size\"] <= max_size and\n",
    "            specs[\"inference_time\"] <= max_inference_time):\n",
    "            selected_model = model\n",
    "            break\n",
    "    \n",
    "    return selected_model\n",
    "\n",
    "# Example usage\n",
    "task = \"classification\"\n",
    "performance_threshold = 0.88\n",
    "max_size = 1500  # MB\n",
    "max_inference_time = 250  # ms\n",
    "\n",
    "selected_model = select_model(task, performance_threshold, max_size, max_inference_time)\n",
    "print(f\"Selected Model: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6aaa2",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (2 minutes)\n",
    "\n",
    "We've covered various evaluation metrics for different NLP tasks, including classification, generation, and language modeling. Remember, the choice of metric depends on your specific task and requirements. Always consider multiple metrics for a comprehensive evaluation of your model's performance.\n",
    "\n",
    "Are there any questions about the evaluation methods or metrics we've discussed?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"BLEU: a Method for Automatic Evaluation of Machine Translation\" paper: https://www.aclweb.org/anthology/P02-1040.pdf\n",
    "2. \"ROUGE: A Package for Automatic Evaluation of Summaries\" paper: https://www.aclweb.org/anthology/W04-1013.pdf\n",
    "3. Hugging Face's Evaluate library: https://huggingface.co/docs/evaluate/index\n",
    "4. scikit-learn metrics documentation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "In our next lesson, we'll dive into practical aspects of model inference and function calling."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
