{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b426c574",
   "metadata": {},
   "source": [
    "# Lesson 12: Model Inference and Function Calling\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Inference and Function Calling. In this 60-minute session, we'll explore practical aspects of using Large Language Models (LLMs), including loading and using local models, calling remote APIs, and working with the JAIS model.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Load and use local LLM models using PyTorch and Hugging Face\n",
    "2. Estimate model size and manage GPU resources\n",
    "3. Use the OpenAI API to access remote LLM services\n",
    "4. Implement inference using the JAIS model\n",
    "\n",
    "## 1. Using PyTorch/HuggingFace to Load and Use Local LLM Models (25 minutes)\n",
    "\n",
    "### 1.1 Loading a Pre-trained Model (10 minutes)\n",
    "\n",
    "Let's start by loading a pre-trained model using the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load GPT-2 as an example\n",
    "model_name = \"gpt2\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model size: {model.num_parameters()} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a22c4",
   "metadata": {},
   "source": [
    "### 1.2 Estimating Model Size and GPU Memory (5 minutes)\n",
    "\n",
    "It's crucial to understand the memory requirements of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "model_size_mb = estimate_model_size(model)\n",
    "print(f\"Estimated model size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "    print(f\"Total GPU memory: {gpu_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350773ad",
   "metadata": {},
   "source": [
    "### 1.3 Configuring GPU Usage (5 minutes)\n",
    "\n",
    "If you have multiple GPUs, you can specify which one to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_gpu(gpu_id):\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        print(f\"Using GPU: {gpu_id}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "# Example: Use GPU 0\n",
    "device = set_gpu(0)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32034da",
   "metadata": {},
   "source": [
    "### 1.4 Model Inference (5 minutes)\n",
    "\n",
    "Now, let's perform inference with our loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"In the future, artificial intelligence will\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366933f",
   "metadata": {},
   "source": [
    "## 2. Using OpenAI API to Call LLM Remote Service (15 minutes)\n",
    "\n",
    "OpenAI's API provides access to powerful language models like GPT-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "def generate_text_openai(prompt, model=\"text-davinci-002\", max_tokens=50):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "generated_text = generate_text_openai(prompt)\n",
    "print(f\"OpenAI API response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69f5da",
   "metadata": {},
   "source": [
    "### 2.1 Error Handling and Rate Limiting (5 minutes)\n",
    "\n",
    "When working with remote APIs, it's important to handle errors and respect rate limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687043b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_text_openai_with_retry(prompt, model=\"text-davinci-002\", max_tokens=50, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return generate_text_openai(prompt, model, max_tokens)\n",
    "        except openai.error.RateLimitError:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "        except openai.error.OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage with retry\n",
    "generated_text = generate_text_openai_with_retry(prompt)\n",
    "if generated_text:\n",
    "    print(f\"OpenAI API response (with retry): {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a527ea3",
   "metadata": {},
   "source": [
    "## 3. Demo: Using the JAIS Model (15 minutes)\n",
    "\n",
    "The JAIS model is a powerful Arabic language model. While we don't have direct access to it, we can demonstrate how you might use it if it were available through a similar interface as other Hugging Face models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jais_model():\n",
    "    # This is a placeholder function. In reality, you would need the actual model files and possibly special tokenizer.\n",
    "    model_name = \"jais-model\"  # This would be the actual model name or path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_text_jais(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage (this is conceptual and won't actually run without the JAIS model)\n",
    "jais_tokenizer, jais_model = load_jais_model()\n",
    "arabic_prompt = \"ترجم النص التالي إلى اللغة الإنجليزية: 'مرحبا، كيف حالك؟'\"\n",
    "generated_text = generate_text_jais(jais_model, jais_tokenizer, arabic_prompt)\n",
    "print(f\"JAIS model response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d5e0b",
   "metadata": {},
   "source": [
    "Note: The above code is conceptual. To actually use the JAIS model, you would need access to the model files and possibly a special tokenizer designed for Arabic text.\n",
    "\n",
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've covered how to load and use local LLM models, estimate their size and manage GPU resources, use the OpenAI API for remote inference, and conceptually how to work with the JAIS model. These skills form the foundation for implementing LLMs in various applications.\n",
    "\n",
    "Are there any questions about model inference or function calling?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. Hugging Face Transformers documentation: https://huggingface.co/transformers/\n",
    "2. PyTorch CUDA semantics: https://pytorch.org/docs/stable/notes/cuda.html\n",
    "3. OpenAI API documentation: https://beta.openai.com/docs/\n",
    "4. JAIS model information (if available, please provide the official source)\n",
    "\n",
    "In our next lesson, we'll explore advanced techniques in prompt engineering to optimize our interactions with these powerful language models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
