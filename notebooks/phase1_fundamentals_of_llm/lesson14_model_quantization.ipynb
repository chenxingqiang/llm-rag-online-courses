{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bf76fc",
   "metadata": {},
   "source": [
    "# Lesson 14: Model Quantization Techniques\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Quantization Techniques. In this 60-minute session, we'll explore the importance of quantization in deploying large language models, different quantization methods, and their practical implementation.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand why quantization is necessary for LLMs\n",
    "2. Differentiate between symmetric and asymmetric quantization\n",
    "3. Comprehend the differences between online and offline quantization\n",
    "4. Practically apply quantization to a model and evaluate its impact\n",
    "\n",
    "## 1. Why Quantization is Necessary (10 minutes)\n",
    "\n",
    "Model quantization is the process of reducing the precision of the model's weights and activations. It's necessary for several reasons:\n",
    "\n",
    "a) Reduced Model Size:\n",
    "   - Smaller storage requirements\n",
    "   - Easier deployment on edge devices\n",
    "\n",
    "b) Faster Inference:\n",
    "   - Reduced memory bandwidth\n",
    "   - More efficient computations, especially on specialized hardware\n",
    "\n",
    "c) Energy Efficiency:\n",
    "   - Lower power consumption, crucial for mobile and IoT devices\n",
    "\n",
    "d) Enabling Deployment on Resource-Constrained Devices:\n",
    "   - Makes it possible to run models on devices with limited memory or processing power\n",
    "\n",
    "Let's see an example of model size before quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "print(f\"Model size: {get_model_size(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80252b8",
   "metadata": {},
   "source": [
    "## 2. Symmetric vs. Asymmetric Quantization (15 minutes)\n",
    "\n",
    "### Symmetric Quantization:\n",
    "- Uses a single scale factor for both positive and negative values\n",
    "- Zero-point is typically fixed at 0\n",
    "- Simpler to implement and compute\n",
    "\n",
    "### Asymmetric Quantization:\n",
    "- Uses both a scale factor and a zero-point\n",
    "- Can represent the original distribution more accurately\n",
    "- Slightly more complex computations\n",
    "\n",
    "Let's implement a simple symmetric quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def symmetric_quantize(tensor, num_bits=8):\n",
    "    qmin = -(2.0 ** (num_bits - 1))\n",
    "    qmax = 2.0 ** (num_bits - 1) - 1\n",
    "    scale = max(torch.abs(tensor.min()), torch.abs(tensor.max())) / qmax\n",
    "    \n",
    "    quantized = torch.clamp(torch.round(tensor / scale), qmin, qmax)\n",
    "    return quantized, scale\n",
    "\n",
    "# Example usage\n",
    "original_tensor = torch.randn(1000)\n",
    "quantized_tensor, scale = symmetric_quantize(original_tensor)\n",
    "\n",
    "print(f\"Original tensor range: [{original_tensor.min():.4f}, {original_tensor.max():.4f}]\")\n",
    "print(f\"Quantized tensor range: [{quantized_tensor.min():.4f}, {quantized_tensor.max():.4f}]\")\n",
    "print(f\"Scale factor: {scale:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9f0fc",
   "metadata": {},
   "source": [
    "## 3. Online vs. Offline Quantization (15 minutes)\n",
    "\n",
    "### Offline Quantization:\n",
    "- Performed during or after training, before deployment\n",
    "- Uses a representative dataset to determine quantization parameters\n",
    "- Resulting model is fully quantized at inference time\n",
    "\n",
    "### Online (Dynamic) Quantization:\n",
    "- Performed at runtime, during inference\n",
    "- Adapts to the specific input data\n",
    "- Can be more flexible but may have higher computational overhead\n",
    "\n",
    "Let's implement a simple offline quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def offline_quantize_model(model, num_bits=8):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            quantized_param, scale = symmetric_quantize(param.data, num_bits)\n",
    "            param.data = quantized_param * scale\n",
    "    return model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "original_size = get_model_size(model)\n",
    "\n",
    "quantized_model = offline_quantize_model(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e847b",
   "metadata": {},
   "source": [
    "## 4. Practical Exercise: Quantize a Model and Evaluate Impact (15 minutes)\n",
    "\n",
    "Let's quantize a pre-trained model and evaluate its impact on size and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Original model inference\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "original_output = model.generate(input_ids, max_length=50)\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Quantize model\n",
    "quantized_model = offline_quantize_model(model)\n",
    "\n",
    "# Quantized model inference\n",
    "quantized_output = quantized_model.generate(input_ids, max_length=50)\n",
    "quantized_text = tokenizer.decode(quantized_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Original output:\", original_text)\n",
    "print(\"Quantized output:\", quantized_text)\n",
    "\n",
    "# Compare sizes\n",
    "original_size = get_model_size(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985a8ac",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've explored the theory behind model quantization and applied it practically to a GPT-2 model. We've seen how quantization can significantly reduce model size while potentially maintaining similar output quality. However, it's important to note that more sophisticated quantization techniques and careful evaluation are needed for production-ready quantized models.\n",
    "\n",
    "Are there any questions about the concepts we've covered or the practical exercise?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\" paper: https://arxiv.org/abs/1712.05877\n",
    "2. PyTorch Quantization Documentation: https://pytorch.org/docs/stable/quantization.html\n",
    "3. \"A Survey of Quantization Methods for Efficient Neural Network Inference\" paper: https://arxiv.org/abs/2103.13630\n",
    "4. Hugging Face's Optimum Library for model optimization: https://huggingface.co/docs/optimum/index\n",
    "\n",
    "In our next lesson, we'll dive into the practical aspects of building a chatbot system based on the LLM techniques we've learned."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
