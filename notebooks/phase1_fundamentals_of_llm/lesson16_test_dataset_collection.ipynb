{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296e8ce9",
   "metadata": {},
   "source": [
    "# Lesson 16: Test Dataset Collection and Model Evaluation\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Test Dataset Collection and Model Evaluation. In this 60-minute session, we'll explore the crucial processes of collecting and preparing data for our chatbot project, as well as evaluating the performance of our language model.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Collect and prepare a suitable dataset for chatbot training and evaluation\n",
    "2. Implement data cleaning and preprocessing techniques\n",
    "3. Understand and apply various model evaluation metrics\n",
    "4. Conduct a comprehensive evaluation of a language model's performance\n",
    "\n",
    "## 1. Data Collection for Chatbot Applications (15 minutes)\n",
    "\n",
    "### 1.1 Sources of Data\n",
    "\n",
    "- Public datasets (e.g., Reddit conversations, movie dialogues)\n",
    "- Customer service logs\n",
    "- Synthetic data generation\n",
    "- Crowdsourcing\n",
    "\n",
    "### 1.2 Data Collection Process\n",
    "\n",
    "Let's implement a simple data collector using the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9103cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "def collect_reddit_data(subreddit_name, limit=1000):\n",
    "    reddit = praw.Reddit(client_id='YOUR_CLIENT_ID',\n",
    "                         client_secret='YOUR_CLIENT_SECRET',\n",
    "                         user_agent='YOUR_USER_AGENT')\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    data = []\n",
    "    \n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.parent_id.startswith('t1_'):  # Ensure it's a reply\n",
    "                parent = reddit.comment(comment.parent_id[3:])\n",
    "                data.append({\n",
    "                    'context': parent.body,\n",
    "                    'response': comment.body\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Usage\n",
    "df = collect_reddit_data('casualconversation')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228651b",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing (15 minutes)\n",
    "\n",
    "### 2.1 Low-quality Filtering\n",
    "\n",
    "Remove irrelevant or low-quality data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7af908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text.strip()\n",
    "\n",
    "def filter_data(df):\n",
    "    # Remove short or long messages\n",
    "    df = df[(df['context'].str.len() > 5) & (df['context'].str.len() < 200)]\n",
    "    df = df[(df['response'].str.len() > 5) & (df['response'].str.len() < 200)]\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df['context'] = df['context'].apply(clean_text)\n",
    "    df['response'] = df['response'].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_df = filter_data(df)\n",
    "print(f\"Original size: {len(df)}, Cleaned size: {len(cleaned_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f8d13",
   "metadata": {},
   "source": [
    "### 2.2 Redundancy Handling\n",
    "\n",
    "Remove duplicate or near-duplicate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19921944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def remove_duplicates(df, threshold=0.9):\n",
    "    tfidf = TfidfVectorizer().fit_transform(df['context'] + \" \" + df['response'])\n",
    "    cosine_sim = cosine_similarity(tfidf, tfidf)\n",
    "    \n",
    "    to_remove = set()\n",
    "    for i in range(len(df)):\n",
    "        if i not in to_remove:\n",
    "            similar = np.where(cosine_sim[i] > threshold)[0]\n",
    "            to_remove.update(similar[similar != i])\n",
    "    \n",
    "    return df.drop(df.index[list(to_remove)])\n",
    "\n",
    "deduped_df = remove_duplicates(cleaned_df)\n",
    "print(f\"Size after deduplication: {len(deduped_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c083e",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation Metrics (15 minutes)\n",
    "\n",
    "For chatbot and language model evaluation, we'll consider several metrics:\n",
    "\n",
    "### 3.1 Perplexity\n",
    "\n",
    "Perplexity measures how well a probability model predicts a sample. Lower perplexity indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(outputs.loss).item()\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "perplexity = calculate_perplexity(model, tokenizer, sample_text)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2967f",
   "metadata": {},
   "source": [
    "### 3.2 BLEU Score\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) score is used to evaluate the quality of machine-translated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa571ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return sentence_bleu([reference.split()], candidate.split())\n",
    "\n",
    "reference = \"The cat is on the mat\"\n",
    "candidate = \"There is a cat on the mat\"\n",
    "bleu_score = calculate_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47584c",
   "metadata": {},
   "source": [
    "### 3.3 Human Evaluation\n",
    "\n",
    "While automated metrics are useful, human evaluation is crucial for assessing the quality and relevance of chatbot responses. Consider factors like:\n",
    "\n",
    "- Relevance\n",
    "- Coherence\n",
    "- Engagement\n",
    "- Factual accuracy\n",
    "\n",
    "## 4. Conducting a Comprehensive Evaluation (10 minutes)\n",
    "\n",
    "Let's put everything together to evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40664eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_data):\n",
    "    perplexities = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        context = row['context']\n",
    "        true_response = row['response']\n",
    "        \n",
    "        # Generate model response\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs, max_length=50)\n",
    "        predicted_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        perplexity = calculate_perplexity(model, tokenizer, context)\n",
    "        bleu = calculate_bleu(true_response, predicted_response)\n",
    "        \n",
    "        perplexities.append(perplexity)\n",
    "        bleu_scores.append(bleu)\n",
    "    \n",
    "    return {\n",
    "        \"avg_perplexity\": sum(perplexities) / len(perplexities),\n",
    "        \"avg_bleu\": sum(bleu_scores) / len(bleu_scores)\n",
    "    }\n",
    "\n",
    "# Assuming we have our test_data DataFrame\n",
    "results = evaluate_model(model, tokenizer, test_data)\n",
    "print(f\"Average Perplexity: {results['avg_perplexity']}\")\n",
    "print(f\"Average BLEU Score: {results['avg_bleu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfadfe",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "In this lesson, we've covered the entire process of collecting and preparing a dataset for our chatbot project, as well as evaluating the performance of our language model. Remember that while automated metrics are useful, they should be complemented with human evaluation for a comprehensive assessment.\n",
    "\n",
    "Are there any questions about the data collection process or evaluation metrics?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Evaluation of Text Generation: A Survey\" paper: https://arxiv.org/abs/2006.14799\n",
    "2. NLTK documentation for BLEU score: https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "3. \"A Survey of Evaluation Techniques for Dialogue Systems\" paper: https://arxiv.org/abs/1905.04071\n",
    "4. Hugging Face Datasets library: https://huggingface.co/docs/datasets/\n",
    "\n",
    "In our next lesson, we'll focus on designing input and output formats for our chatbot with context management."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
