{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13c520a",
   "metadata": {},
   "source": [
    "# Lesson 18: Model Deployment and Backend Development\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Deployment and Backend Development. In this 60-minute session, we'll explore how to deploy a trained language model and develop a robust backend system for our chatbot.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Prepare a trained model for deployment\n",
    "2. Containerize the model using Docker\n",
    "3. Develop a backend API using Flask\n",
    "4. Implement model inference in the backend\n",
    "5. Understand basic security and scalability considerations\n",
    "\n",
    "## 1. Preparing the Model for Deployment (15 minutes)\n",
    "\n",
    "Before deploying, we need to ensure our model is optimized and in the right format.\n",
    "\n",
    "### 1.1 Model Conversion\n",
    "\n",
    "We'll use ONNX (Open Neural Network Exchange) format for better performance and cross-platform support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def convert_to_onnx(model_name, output_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_input = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(model, dummy_input, output_path, \n",
    "                      input_names=['input_ids'], \n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                                    'output': {0: 'batch_size', 1: 'sequence'}},\n",
    "                      opset_version=11)\n",
    "\n",
    "    print(f\"Model converted and saved to {output_path}\")\n",
    "\n",
    "# Usage\n",
    "convert_to_onnx(\"gpt2\", \"gpt2_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4305d93",
   "metadata": {},
   "source": [
    "### 1.2 Model Quantization\n",
    "\n",
    "To reduce model size and improve inference speed, we can apply quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic\n",
    "\n",
    "def quantize_onnx_model(model_path, quantized_model_path):\n",
    "    quantize_dynamic(model_path, quantized_model_path)\n",
    "    print(f\"Quantized model saved to {quantized_model_path}\")\n",
    "\n",
    "# Usage\n",
    "quantize_onnx_model(\"gpt2_model.onnx\", \"gpt2_model_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3209c6e",
   "metadata": {},
   "source": [
    "## 2. Containerizing the Model (15 minutes)\n",
    "\n",
    "We'll use Docker to containerize our model for easy deployment.\n",
    "\n",
    "Create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.8-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY gpt2_model_quantized.onnx .\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928df5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a `requirements.txt`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b06cd",
   "metadata": {},
   "source": [
    "flask\n",
    "onnxruntime\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the Docker image:\n",
    "\n",
    "```bash\n",
    "docker build -t chatbot-model ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f8a06",
   "metadata": {},
   "source": [
    "Run the container:\n",
    "\n",
    "```bash\n",
    "docker run -p 5000:5000 chatbot-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Developing the Backend API (20 minutes)\n",
    "\n",
    "We'll use Flask to create a simple API for our chatbot.\n",
    "\n",
    "Create `app.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08373a64",
   "metadata": {},
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "ort_session = ort.InferenceSession(\"gpt2_model_quantized.onnx\")\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    user_input = data['input']\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"np\")\n",
    "    \n",
    "    # Run inference\n",
    "    output = ort_session.run(None, {\"input_ids\": input_ids})\n",
    "    \n",
    "    # Decode output\n",
    "    response = tokenizer.decode(output[0][0], skip_special_tokens=True)\n",
    "    \n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336df5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Implementing Model Inference (10 minutes)\n",
    "\n",
    "Let's expand our `/chat` endpoint to handle context and implement more sophisticated inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f56d1d",
   "metadata": {},
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "ort_session = ort.InferenceSession(\"gpt2_model_quantized.onnx\")\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    user_input = data['input']\n",
    "    context = data.get('context', [])\n",
    "    \n",
    "    # Prepare input with context\n",
    "    full_input = \" \".join(context + [user_input])\n",
    "    input_ids = tokenizer.encode(full_input, return_tensors=\"np\")\n",
    "    \n",
    "    # Run inference\n",
    "    output = ort_session.run(None, {\"input_ids\": input_ids})\n",
    "    \n",
    "    # Generate response\n",
    "    generated = np.argmax(output[0], axis=-1)\n",
    "    response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the new generated text\n",
    "    response = response[len(full_input):].strip()\n",
    "    \n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Security and Scalability Considerations (5 minutes)\n",
    "\n",
    "1. Security:\n",
    "   - Implement API authentication (e.g., JWT tokens)\n",
    "   - Use HTTPS for all communications\n",
    "   - Sanitize and validate all user inputs\n",
    "   - Regularly update dependencies\n",
    "\n",
    "2. Scalability:\n",
    "   - Use a production-grade WSGI server (e.g., Gunicorn)\n",
    "   - Implement load balancing\n",
    "   - Consider serverless deployments for automatic scaling\n",
    "   - Optimize database queries and implement caching\n",
    "\n",
    "Example of adding basic authentication:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a88f8",
   "metadata": {},
   "source": [
    "from flask_httpauth import HTTPBasicAuth\n",
    "from werkzeug.security import generate_password_hash, check_password_hash\n",
    "\n",
    "auth = HTTPBasicAuth()\n",
    "\n",
    "users = {\n",
    "    \"admin\": generate_password_hash(\"secret\")\n",
    "}\n",
    "\n",
    "@auth.verify_password\n",
    "def verify_password(username, password):\n",
    "    if username in users and check_password_hash(users.get(username), password):\n",
    "        return username\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "@auth.login_required\n",
    "def chat():\n",
    "    # ... (previous chat function code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70495377",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "In this lesson, we've covered the essential steps for deploying a language model and developing a backend API for our chatbot. We've explored model optimization, containerization, API development, and touched on important security and scalability considerations.\n",
    "\n",
    "Are there any questions about model deployment or backend development?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. ONNX Runtime documentation: https://onnxruntime.ai/docs/\n",
    "2. Flask documentation: https://flask.palletsprojects.com/\n",
    "3. Docker documentation: https://docs.docker.com/\n",
    "4. \"Designing Data-Intensive Applications\" by Martin Kleppmann (for scalability concepts)\n",
    "5. OWASP API Security Top 10: https://owasp.org/www-project-api-security/\n",
    "\n",
    "In our next lesson, we'll focus on developing the frontend interface for our chatbot system."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
