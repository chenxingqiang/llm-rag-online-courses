{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a566e564",
   "metadata": {},
   "source": [
    "# Lesson 2: NLP Fundamentals\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our deep dive into Natural Language Processing (NLP) fundamentals. In this hour-long session, we'll explore why NLP is crucial in modern AI, what we can achieve with it, and how NLP algorithms and models have evolved over time.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the importance of NLP in AI applications\n",
    "2. Recognize the main types of NLP tasks\n",
    "3. Comprehend the evolution of NLP algorithms and models\n",
    "4. Gain insights into state-of-the-art NLP models\n",
    "\n",
    "## 1. Why We Need NLP and What We Can Do with It (15 minutes)\n",
    "\n",
    "### Importance of NLP:\n",
    "1. Human-Computer Interaction: Enables more natural communication between humans and machines.\n",
    "2. Information Extraction: Helps in deriving structured information from unstructured text data.\n",
    "3. Automation: Automates various language-related tasks, saving time and resources.\n",
    "4. Accessibility: Makes information more accessible across languages and for people with disabilities.\n",
    "5. Decision Making: Assists in data-driven decision making by analyzing textual information.\n",
    "\n",
    "### Main NLP Tasks:\n",
    "\n",
    "#### a. Classification Tasks (5 minutes)\n",
    "- Sentiment Analysis\n",
    "- Spam Detection\n",
    "- Topic Classification\n",
    "- Language Identification\n",
    "\n",
    "Example: Sentiment Analysis using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"I love this NLP course! It's very informative.\"\n",
    "sentiment = sia.polarity_scores(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bc5a0",
   "metadata": {},
   "source": [
    "#### b. Extraction Tasks (5 minutes)\n",
    "- Named Entity Recognition (NER)\n",
    "- Relation Extraction\n",
    "- Key Phrase Extraction\n",
    "- Information Retrieval\n",
    "\n",
    "Example: Named Entity Recognition using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7faca",
   "metadata": {},
   "source": [
    "#### c. Generation Tasks (5 minutes)\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Dialogue Systems\n",
    "- Text Completion\n",
    "\n",
    "Example: Text Generation using GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f279ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"The future of NLP is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e590334",
   "metadata": {},
   "source": [
    "## 2. Evolution of NLP Algorithms and Models (35 minutes)\n",
    "\n",
    "### a. Bag-of-Words Model (5 minutes)\n",
    "- Simple representation of text as an unordered set of words\n",
    "- Ignores grammar and word order\n",
    "- Used in document classification and information retrieval\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3020498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Natural language processing is fascinating.\",\n",
    "          \"NLP models have evolved significantly.\",\n",
    "          \"Modern NLP uses advanced neural networks.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad390f51",
   "metadata": {},
   "source": [
    "### b. Word2Vec (5 minutes)\n",
    "- Introduced by Google in 2013\n",
    "- Creates dense vector representations of words\n",
    "- Captures semantic relationships between words\n",
    "\n",
    "Example using Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f91bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"natural\", \"language\", \"processing\"],\n",
    "             [\"nlp\", \"models\", \"have\", \"evolved\"],\n",
    "             [\"modern\", \"nlp\", \"uses\", \"neural\", \"networks\"]]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(model.wv.most_similar(\"nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf3dc2",
   "metadata": {},
   "source": [
    "### c. LSTM (Long Short-Term Memory) (5 minutes)\n",
    "- A type of recurrent neural network (RNN)\n",
    "- Capable of learning long-term dependencies\n",
    "- Widely used in sequence-to-sequence tasks like machine translation\n",
    "\n",
    "### d. Transformer (5 minutes)\n",
    "- Introduced in the paper \"Attention Is All You Need\" (2017)\n",
    "- Uses self-attention mechanism\n",
    "- Allows for more parallelization during training\n",
    "\n",
    "### e. BERT (Bidirectional Encoder Representations from Transformers) (5 minutes)\n",
    "- Introduced by Google in 2018\n",
    "- Pre-trained on a large corpus of unlabeled text\n",
    "- Achieves state-of-the-art results on many NLP tasks\n",
    "\n",
    "Example: Using BERT for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904012dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"I love NLP!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(f\"Positive sentiment probability: {prediction[0][1].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5522cb",
   "metadata": {},
   "source": [
    "### f. T5 (Text-to-Text Transfer Transformer) (5 minutes)\n",
    "- Introduced by Google in 2019\n",
    "- Treats every NLP task as a \"text-to-text\" problem\n",
    "- Versatile model that can be fine-tuned for various tasks\n",
    "\n",
    "### g. GPT-2 (Generative Pre-trained Transformer 2) (5 minutes)\n",
    "- Introduced by OpenAI in 2019\n",
    "- Large transformer-based language model\n",
    "- Known for its impressive text generation capabilities\n",
    "\n",
    "## 3. Comparison and Future Trends (5 minutes)\n",
    "\n",
    "- Discussion on the strengths and weaknesses of different models\n",
    "- Current state-of-the-art in NLP\n",
    "- Future directions and challenges in NLP research\n",
    "\n",
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've covered the fundamentals of NLP, including its importance, main tasks, and the evolution of algorithms and models. As we progress through this course, we'll delve deeper into these concepts and learn how to apply them in real-world scenarios.\n",
    "\n",
    "Are there any questions about the topics we've covered?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Speech and Language Processing\" by Dan Jurafsky and James H. Martin\n",
    "2. \"Natural Language Processing with Transformers\" by Lewis Tunstall, Leandro von Werra, and Thomas Wolf\n",
    "3. Original BERT paper: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
    "4. Original Transformer paper: \"Attention Is All You Need\"\n",
    "\n",
    "In our next lesson, we'll explore the basic knowledge and architectural characteristics of Large Language Models in more detail."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
