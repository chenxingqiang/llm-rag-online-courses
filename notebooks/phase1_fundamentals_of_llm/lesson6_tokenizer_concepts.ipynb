{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d1b2d5",
   "metadata": {},
   "source": [
    "# Lesson 6: The Concept of the Tokenizer and Common Types\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on tokenizers. In this hour-long session, we'll explore the fundamental concept of tokenization in Natural Language Processing (NLP) and Large Language Models (LLMs), its importance, and the most common types of tokenizers used in modern NLP tasks.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the concept of tokenization and its role in LLMs\n",
    "2. Recognize the importance of tokenization in NLP tasks\n",
    "3. Explore common types of tokenizers (Word-based, Character-based, Subword-based)\n",
    "4. Gain hands-on experience with different tokenization techniques\n",
    "\n",
    "## 1. The Concept of Tokenizer and its Role in LLM (15 minutes)\n",
    "\n",
    "### What is a Tokenizer?\n",
    "\n",
    "A tokenizer is a crucial component in NLP that breaks down text into smaller units called tokens. These tokens serve as the basic units of meaning that models can process.\n",
    "\n",
    "### Why is Tokenization Important?\n",
    "\n",
    "1. Input Preparation: Converts raw text into a format that models can understand\n",
    "2. Vocabulary Management: Helps in creating and maintaining a fixed vocabulary\n",
    "3. Out-of-Vocabulary (OOV) Handling: Addresses words not seen during training\n",
    "4. Language Agnostic Processing: Enables models to work with multiple languages\n",
    "\n",
    "### Tokenization in the LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18775c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is a crucial step in NLP!\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(inputs.input_ids)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90935be1",
   "metadata": {},
   "source": [
    "## 2. Common Types of Tokenizers (30 minutes)\n",
    "\n",
    "### 2.1 Word-based Tokenizers (10 minutes)\n",
    "\n",
    "Word-based tokenizers split text into words, typically using whitespace and punctuation as delimiters.\n",
    "\n",
    "Pros:\n",
    "- Intuitive and easy to understand\n",
    "- Preserves word boundaries\n",
    "\n",
    "Cons:\n",
    "- Large vocabulary size\n",
    "- Poor handling of OOV words\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Word-based tokenization splits text into words.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9740e",
   "metadata": {},
   "source": [
    "### 2.2 Character-based Tokenizers (10 minutes)\n",
    "\n",
    "Character-based tokenizers split text into individual characters.\n",
    "\n",
    "Pros:\n",
    "- Very small vocabulary size\n",
    "- No OOV issues\n",
    "\n",
    "Cons:\n",
    "- Loses word-level semantics\n",
    "- Produces very long sequences\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f95870",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Character-based tokenization.\"\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca92fb2",
   "metadata": {},
   "source": [
    "### 2.3 Subword Tokenizers (10 minutes)\n",
    "\n",
    "Subword tokenizers find a balance between word-level and character-level tokenization by breaking words into meaningful subword units.\n",
    "\n",
    "Common subword tokenization algorithms:\n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- SentencePiece\n",
    "\n",
    "Pros:\n",
    "- Balances vocabulary size and semantic meaning\n",
    "- Handles OOV words effectively\n",
    "- Works well for morphologically rich languages\n",
    "\n",
    "Cons:\n",
    "- More complex than word or character tokenization\n",
    "- May split common words in unintuitive ways\n",
    "\n",
    "Example using Hugging Face Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Create and train a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Training data (in a real scenario, this would be a large corpus)\n",
    "training_data = [\n",
    "    \"Subword tokenization is powerful for handling unknown words.\",\n",
    "    \"It breaks words into meaningful subword units.\"\n",
    "]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(training_data, trainer)\n",
    "\n",
    "# Tokenize some text\n",
    "text = \"Subword tokenizers handle unknown words effectively.\"\n",
    "output = tokenizer.encode(text)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07010b4f",
   "metadata": {},
   "source": [
    "## 3. Practical Exercise: Comparing Tokenizers (10 minutes)\n",
    "\n",
    "Let's compare the output of different tokenizers on the same piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. It's a pangram!\"\n",
    "\n",
    "# Word-based tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Character-based tokenization\n",
    "char_tokens = list(text)\n",
    "\n",
    "# Subword tokenization (using GPT-2 tokenizer as an example)\n",
    "subword_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "subword_tokens = subword_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Word tokens:\", word_tokens)\n",
    "print(\"Character tokens:\", char_tokens)\n",
    "print(\"Subword tokens:\", subword_tokens)\n",
    "\n",
    "print(f\"Number of word tokens: {len(word_tokens)}\")\n",
    "print(f\"Number of character tokens: {len(char_tokens)}\")\n",
    "print(f\"Number of subword tokens: {len(subword_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fe4ed",
   "metadata": {},
   "source": [
    "Discuss the differences in output and the implications for model training and inference.\n",
    "\n",
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've covered the fundamental concept of tokenization, its importance in NLP and LLMs, and explored different types of tokenizers. Remember, the choice of tokenizer can significantly impact model performance and should be considered carefully based on your specific task and language requirements.\n",
    "\n",
    "Are there any questions about the topics we've covered?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Subword Tokenization\" chapter in \"Natural Language Processing with Transformers\" book\n",
    "2. Hugging Face Tokenizers library documentation: https://huggingface.co/docs/tokenizers/\n",
    "3. \"BPE-Dropout: Simple and Effective Subword Regularization\" paper: https://arxiv.org/abs/1910.13267\n",
    "4. SentencePiece paper: https://arxiv.org/abs/1808.06226\n",
    "\n",
    "In our next lesson, we'll dive deeper into data preparation and preprocessing techniques for training LLMs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
