{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6096e2c7",
   "metadata": {},
   "source": [
    "# Lesson 7: Text Data Preprocessing and Preparation\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on text data preprocessing and preparation. In this 90-minute session, we'll explore the crucial steps involved in preparing text data for Large Language Model (LLM) training. We'll cover various techniques for data collection, cleaning, normalization, and augmentation.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the importance of data preprocessing in LLM training\n",
    "2. Learn common methods for text data preprocessing\n",
    "3. Explore techniques for handling large-scale text data\n",
    "4. Gain familiarity with popular pre-training and fine-tuning datasets\n",
    "\n",
    "## 1. Introduction to LLM Training Data Forms (10 minutes)\n",
    "\n",
    "LLMs require vast amounts of text data for training. Common forms of training data include:\n",
    "\n",
    "1. Web crawled data (e.g., Common Crawl)\n",
    "2. Books and literature\n",
    "3. Scientific papers and articles\n",
    "4. Social media posts\n",
    "5. Dialogue data (for conversational models)\n",
    "\n",
    "Example of accessing web crawled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract text from paragraphs\n",
    "text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "print(text[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd72b90",
   "metadata": {},
   "source": [
    "## 2. Common Methods for Text Data Preprocessing (40 minutes)\n",
    "\n",
    "### 2.1 Low-quality Filtering (10 minutes)\n",
    "\n",
    "Removing or filtering out low-quality data is crucial for maintaining the quality of your training set.\n",
    "\n",
    "Techniques include:\n",
    "- Removing duplicate content\n",
    "- Filtering out non-natural language text (e.g., code, random strings)\n",
    "- Removing content with high perplexity scores\n",
    "\n",
    "Example of removing duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ce59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(texts):\n",
    "    return list(dict.fromkeys(texts))\n",
    "\n",
    "texts = [\"Hello world\", \"Hello world\", \"This is unique\", \"Hello world\"]\n",
    "unique_texts = remove_duplicates(texts)\n",
    "print(unique_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a48048",
   "metadata": {},
   "source": [
    "### 2.2 Redundancy Handling (10 minutes)\n",
    "\n",
    "Handling redundancy involves identifying and removing or reducing similar content that doesn't add value to the training set.\n",
    "\n",
    "Techniques include:\n",
    "- Near-duplicate detection\n",
    "- Semantic similarity clustering\n",
    "\n",
    "Example using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e320010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_texts(texts, threshold=0.8):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    similar_pairs = []\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i+1, len(texts)):\n",
    "            if cosine_sim[i][j] > threshold:\n",
    "                similar_pairs.append((i, j))\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A fast brown fox leaps above a sleepy canine\",\n",
    "    \"Python is a popular programming language\",\n",
    "    \"Java is widely used in enterprise software development\"\n",
    "]\n",
    "\n",
    "similar_pairs = find_similar_texts(texts)\n",
    "print(\"Similar text pairs:\", similar_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23f2a2",
   "metadata": {},
   "source": [
    "### 2.3 Privacy Removal (10 minutes)\n",
    "\n",
    "Ensuring privacy in training data is crucial. This involves removing or anonymizing personal information.\n",
    "\n",
    "Techniques include:\n",
    "- Named Entity Recognition (NER) for identifying personal information\n",
    "- Regular expressions for detecting patterns like email addresses or phone numbers\n",
    "\n",
    "Example using spaCy for NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ddfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def anonymize_text(text):\n",
    "    doc = nlp(text)\n",
    "    anonymized = text\n",
    "    for ent in reversed(doc.ents):  # Reverse to avoid indexing issues\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "            anonymized = anonymized[:ent.start_char] + \"[\" + ent.label_ + \"]\" + anonymized[ent.end_char:]\n",
    "    return anonymized\n",
    "\n",
    "text = \"John Doe works at Google in New York.\"\n",
    "anonymized_text = anonymize_text(text)\n",
    "print(\"Original:\", text)\n",
    "print(\"Anonymized:\", anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b52c9",
   "metadata": {},
   "source": [
    "### 2.4 Text Normalization (10 minutes)\n",
    "\n",
    "Text normalization involves standardizing text to a common format.\n",
    "\n",
    "Techniques include:\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Expanding contractions\n",
    "- Handling special characters and emojis\n",
    "\n",
    "Example of basic text normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4516ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "text = \"Hello, World!   How's it going?\"\n",
    "normalized_text = normalize_text(text)\n",
    "print(\"Original:\", text)\n",
    "print(\"Normalized:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f092571",
   "metadata": {},
   "source": [
    "## 3. Introduction to Large-scale Text Data Sources (20 minutes)\n",
    "\n",
    "Large-scale text data sources are crucial for training LLMs. Let's explore some popular sources:\n",
    "\n",
    "### 3.1 Common Crawl\n",
    "\n",
    "Common Crawl is a vast repository of web-crawled data.\n",
    "\n",
    "Example of accessing Common Crawl data (conceptual, not runnable here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524004c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warc\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "def fetch_common_crawl_sample():\n",
    "    url = \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/segments/1610703495745.19/warc/CC-MAIN-20210115134348-20210115164348-00000.warc.gz\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    warc_file = warc.WARCFile(fileobj=gzip.GzipFile(fileobj=response.raw))\n",
    "    \n",
    "    for record in warc_file:\n",
    "        if record['WARC-Type'] == 'response':\n",
    "            print(record.url)\n",
    "            print(record.content)\n",
    "            break  # Just print the first record for demonstration\n",
    "\n",
    "# fetch_common_crawl_sample()  # Uncomment to run (may take a while)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b25c2c",
   "metadata": {},
   "source": [
    "### 3.2 Wikipedia Dumps\n",
    "\n",
    "Wikipedia provides regular dumps of its content, which are widely used in NLP tasks.\n",
    "\n",
    "Example of processing a Wikipedia dump (conceptual, not runnable here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe01b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def process_wiki_dump(dump_path):\n",
    "    wiki = WikiCorpus(dump_path)\n",
    "    for text in wiki.get_texts():\n",
    "        yield ' '.join(text)\n",
    "\n",
    "# Usage:\n",
    "# dump_path = \"path_to_wikipedia_dump.xml.bz2\"\n",
    "# for article in process_wiki_dump(dump_path):\n",
    "#     print(article[:100])  # Print first 100 characters of each article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695fe06",
   "metadata": {},
   "source": [
    "## 4. Introduction to Famous Public Pre-training and Fine-tuning Datasets (10 minutes)\n",
    "\n",
    "Let's briefly discuss some popular datasets used for pre-training and fine-tuning LLMs:\n",
    "\n",
    "1. BookCorpus: A large collection of free books\n",
    "2. OpenWebText: Web content extracted from URLs shared on Reddit\n",
    "3. C4 (Colossal Clean Crawled Corpus): A colossal, cleaned version of Common Crawl\n",
    "4. GLUE (General Language Understanding Evaluation): A collection of tasks for evaluating language understanding\n",
    "\n",
    "Example of loading a dataset using Hugging Face Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456cda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GLUE dataset for the CoLA task\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "print(dataset)\n",
    "\n",
    "# Print a sample from the training set\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36287acc",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've covered various aspects of text data preprocessing and preparation, including data cleaning, normalization, and sources of large-scale text data. Remember, the quality and diversity of your training data significantly impact the performance of your LLM.\n",
    "\n",
    "Are there any questions about the topics we've covered?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Natural Language Processing with Python\" by Steven Bird, Ewan Klein, and Edward Loper\n",
    "2. Common Crawl documentation: https://commoncrawl.org/the-data/get-started/\n",
    "3. Hugging Face Datasets documentation: https://huggingface.co/docs/datasets/\n",
    "4. \"Data Cleaning for Natural Language Processing: A Research Survey\" paper: https://arxiv.org/abs/2103.05028\n",
    "\n",
    "In our next lesson, we'll dive into the specifics of LLM training, including fine-tuning techniques."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
