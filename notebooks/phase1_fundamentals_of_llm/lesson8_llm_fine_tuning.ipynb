{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c0078b",
   "metadata": {},
   "source": [
    "# Lesson 8: LLM Training - Fine-tuning\n",
    "\n",
    "## Introduction (2 minutes)\n",
    "\n",
    "Welcome to our lesson on LLM Fine-tuning. In this 30-minute session, we'll explore various techniques for adapting pre-trained language models to specific tasks or domains.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will understand:\n",
    "1. The concept and importance of fine-tuning in LLM development\n",
    "2. Different fine-tuning techniques: LoRA, P-tuning, and Full-parameter fine-tuning\n",
    "3. How to implement these techniques using popular libraries\n",
    "\n",
    "## 1. Fine-tuning: Concept and Importance (5 minutes)\n",
    "\n",
    "Fine-tuning is the process of further training a pre-trained model on a specific dataset or task. It allows us to:\n",
    "- Adapt general language models to specific domains or tasks\n",
    "- Improve performance on downstream tasks\n",
    "- Reduce the need for large-scale training from scratch\n",
    "\n",
    "## 2. LoRA (Low-Rank Adaptation) (8 minutes)\n",
    "\n",
    "LoRA is an efficient fine-tuning technique that significantly reduces the number of trainable parameters.\n",
    "\n",
    "Key points:\n",
    "- Adds low-rank decomposition matrices to existing weights\n",
    "- Freezes pre-trained model parameters\n",
    "- Dramatically reduces memory usage and training time\n",
    "\n",
    "Example using PEFT library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee095ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    r=8,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Model is ready for fine-tuning\n",
    "print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "print(f\"All params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c364a43",
   "metadata": {},
   "source": [
    "## 3. P-tuning (Prompt-based Tuning) (8 minutes)\n",
    "\n",
    "P-tuning is a technique that learns continuous prompts for specific tasks.\n",
    "\n",
    "Key points:\n",
    "- Introduces trainable \"virtual tokens\" in the input\n",
    "- Keeps most of the pre-trained model frozen\n",
    "- Effective for few-shot learning scenarios\n",
    "\n",
    "Conceptual example (not runnable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class PtuningModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_virtual_tokens=20):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_virtual_tokens = num_virtual_tokens\n",
    "        self.virtual_tokens = torch.nn.Parameter(torch.randn(num_virtual_tokens, base_model.config.hidden_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        virtual_tokens = self.virtual_tokens.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        inputs_embeds = self.base_model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([virtual_tokens, inputs_embeds], dim=1)\n",
    "        attention_mask = torch.cat([torch.ones(batch_size, self.num_virtual_tokens).to(attention_mask.device), attention_mask], dim=1)\n",
    "        \n",
    "        outputs = self.base_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        return outputs\n",
    "\n",
    "# Usage\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "p_tuning_model = PtuningModel(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dbeda",
   "metadata": {},
   "source": [
    "## 4. Full-parameter Fine-tuning (5 minutes)\n",
    "\n",
    "Full-parameter fine-tuning involves updating all parameters of the pre-trained model.\n",
    "\n",
    "Key points:\n",
    "- Provides maximum flexibility\n",
    "- Requires more computational resources\n",
    "- Risk of catastrophic forgetting\n",
    "\n",
    "Example using Hugging Face Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9017c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Assume we have train_dataset and eval_dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d88136",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (2 minutes)\n",
    "\n",
    "We've covered three main fine-tuning techniques: LoRA, P-tuning, and full-parameter fine-tuning. Each has its advantages and use cases. The choice depends on your specific task, available computational resources, and desired trade-off between performance and efficiency.\n",
    "\n",
    "Are there any questions about the fine-tuning techniques we've discussed?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. LoRA paper: \"LoRA: Low-Rank Adaptation of Large Language Models\" (https://arxiv.org/abs/2106.09685)\n",
    "2. P-tuning paper: \"GPT Understands, Too\" (https://arxiv.org/abs/2103.10385)\n",
    "3. Hugging Face Fine-tuning tutorial: https://huggingface.co/docs/transformers/training\n",
    "4. PEFT library documentation: https://github.com/huggingface/peft\n",
    "\n",
    "In our next lesson, we'll explore advanced training techniques, including Reward Modeling and Proximal Policy Optimization."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
