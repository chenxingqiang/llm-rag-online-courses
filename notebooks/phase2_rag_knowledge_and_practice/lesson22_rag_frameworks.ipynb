{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f30287",
   "metadata": {},
   "source": [
    "# Lesson 22: RAG Frameworks - Introduction and use of LlamaIndex and LangChain\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on RAG Frameworks, focusing on LlamaIndex and LangChain. In this 60-minute session, we'll explore these powerful tools for implementing Retrieval-Augmented Generation systems and learn how to use them in practice.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Understand the key features and use cases of LlamaIndex and LangChain\n",
    "2. Set up and use LlamaIndex for document indexing and querying\n",
    "3. Implement a basic RAG system using LangChain\n",
    "4. Compare the strengths and use cases of both frameworks\n",
    "\n",
    "## 1. Introduction to LlamaIndex (5 minutes)\n",
    "\n",
    "LlamaIndex (formerly GPT Index) is a data framework designed to help developers build applications using large language models (LLMs) with external data sources.\n",
    "\n",
    "Key features:\n",
    "- Flexible data ingestion from various sources\n",
    "- Advanced indexing and querying capabilities\n",
    "- Integration with popular LLMs\n",
    "- Support for different types of indices (e.g., list, tree, keyword table)\n",
    "\n",
    "## 2. Setting up and Using LlamaIndex (20 minutes)\n",
    "\n",
    "Let's implement a basic RAG system using LlamaIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807da781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "from llama_index import GPTSimpleVectorIndex, Document, SimpleDirectoryReader\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "# Create index\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "# Create query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)]\n",
    ")\n",
    "\n",
    "# Query the system\n",
    "response = query_engine.query(\"What are the main features of LlamaIndex?\")\n",
    "print(response)\n",
    "\n",
    "# Save and load index\n",
    "index.save_to_disk('index.json')\n",
    "loaded_index = GPTSimpleVectorIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633afd5",
   "metadata": {},
   "source": [
    "This example demonstrates how to load documents, create an index, query the system, and save/load the index for future use.\n",
    "\n",
    "## 3. Introduction to LangChain (5 minutes)\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models, focusing on composability and end-to-end solutions.\n",
    "\n",
    "Key features:\n",
    "- Prompt management and optimization\n",
    "- Chain of thought reasoning\n",
    "- Integration with external data sources and APIs\n",
    "- Memory management for conversational applications\n",
    "\n",
    "## 4. Implementing RAG with LangChain (20 minutes)\n",
    "\n",
    "Now, let's implement a RAG system using LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader(\"data/example.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Create retrieval chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the system\n",
    "query = \"What are the main features of LangChain?\"\n",
    "response = qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417a56f",
   "metadata": {},
   "source": [
    "This example shows how to load a document, split it into chunks, create embeddings, set up a vector store, and use a retrieval chain for question-answering.\n",
    "\n",
    "## 5. Comparing LlamaIndex and LangChain (5 minutes)\n",
    "\n",
    "Let's compare these frameworks:\n",
    "\n",
    "1. Focus:\n",
    "   - LlamaIndex: Specialized in data indexing and retrieval\n",
    "   - LangChain: Broader framework for LLM-powered applications\n",
    "\n",
    "2. Ease of Use:\n",
    "   - LlamaIndex: Simpler API for basic RAG tasks\n",
    "   - LangChain: More flexible but potentially more complex\n",
    "\n",
    "3. Features:\n",
    "   - LlamaIndex: Advanced indexing strategies\n",
    "   - LangChain: Comprehensive toolkit for various LLM tasks\n",
    "\n",
    "4. Integration:\n",
    "   - Both integrate well with popular LLMs and vector stores\n",
    "\n",
    "5. Use Cases:\n",
    "   - LlamaIndex: Ideal for document-heavy applications\n",
    "   - LangChain: Suitable for a wider range of LLM applications\n",
    "\n",
    "## Hands-on Exercise (15 minutes)\n",
    "\n",
    "Let's create a simple comparison by implementing the same task in both frameworks:\n",
    "\n",
    "Task: Create a QA system over a set of documents about AI ethics.\n",
    "\n",
    "1. LlamaIndex Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3248f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('ai_ethics_docs').load_data()\n",
    "\n",
    "# Create index\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "# Query\n",
    "response = index.query(\"What are the main ethical concerns in AI development?\")\n",
    "print(\"LlamaIndex Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb15b8",
   "metadata": {},
   "source": [
    "2. LangChain Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader('ai_ethics_docs')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Create QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "# Query\n",
    "response = qa.run(\"What are the main ethical concerns in AI development?\")\n",
    "print(\"LangChain Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1d364",
   "metadata": {},
   "source": [
    "Compare the responses and the implementation process for both frameworks.\n",
    "\n",
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "In this lesson, we've explored two powerful RAG frameworks: LlamaIndex and LangChain. We've seen how to implement basic RAG systems using both frameworks and compared their features and use cases.\n",
    "\n",
    "Are there any questions about LlamaIndex, LangChain, or their applications in RAG systems?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. LlamaIndex Documentation: https://gpt-index.readthedocs.io/\n",
    "2. LangChain Documentation: https://langchain.readthedocs.io/\n",
    "3. \"Building RAG Applications with LlamaIndex\" tutorial: https://medium.com/@jerryjliu98/building-rag-applications-with-llamaindex-54f6c953291a\n",
    "4. \"Getting Started with LangChain\" guide: https://python.langchain.com/docs/get_started/introduction.html\n",
    "\n",
    "In our next lesson, we'll dive deeper into embedding models, a crucial component of RAG systems."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
