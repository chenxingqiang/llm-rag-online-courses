{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae104cbb",
   "metadata": {},
   "source": [
    "# Lesson 23: RAG Embedding Model\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on RAG Embedding Models. In this 60-minute session, we'll explore the crucial role of embedding models in Retrieval-Augmented Generation systems, how to select appropriate models, and how to implement and evaluate them in practice.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Understand the concept of embedding models and their importance in RAG\n",
    "2. Recognize different types of embedding models and their characteristics\n",
    "3. Select appropriate embedding models for specific RAG tasks\n",
    "4. Implement and use embedding models in a RAG system\n",
    "5. Evaluate the performance of embedding models\n",
    "\n",
    "## 1. Introduction to Embedding Models (10 minutes)\n",
    "\n",
    "Embedding models are neural networks that convert text into dense vector representations, capturing semantic meaning in a high-dimensional space.\n",
    "\n",
    "Key points:\n",
    "- Embeddings enable efficient similarity search\n",
    "- They capture semantic relationships between words and documents\n",
    "- Crucial for both indexing documents and encoding queries in RAG systems\n",
    "\n",
    "Types of embedding models:\n",
    "1. Word embeddings (e.g., Word2Vec, GloVe)\n",
    "2. Sentence embeddings (e.g., SBERT, Universal Sentence Encoder)\n",
    "3. Document embeddings (e.g., Doc2Vec)\n",
    "\n",
    "## 2. Importance of Embedding Models in RAG (10 minutes)\n",
    "\n",
    "In RAG systems, embedding models serve two primary purposes:\n",
    "1. Encoding documents for efficient storage and retrieval\n",
    "2. Encoding user queries for similarity matching\n",
    "\n",
    "Benefits of good embedding models in RAG:\n",
    "- Improved retrieval accuracy\n",
    "- Better handling of semantic similarity\n",
    "- Reduced computational cost for large-scale retrieval\n",
    "\n",
    "Example of using embeddings in a simple RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "\n",
    "    def add_document(self, doc):\n",
    "        self.documents.append(doc)\n",
    "        self.embeddings.append(self.model.encode(doc))\n",
    "\n",
    "    def query(self, query, top_k=1):\n",
    "        query_embedding = self.model.encode(query)\n",
    "        scores = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.documents[i] for i in top_indices]\n",
    "\n",
    "# Usage\n",
    "rag = SimpleRAG()\n",
    "rag.add_document(\"Embedding models are crucial for RAG systems.\")\n",
    "rag.add_document(\"RAG combines retrieval and generation for better AI responses.\")\n",
    "\n",
    "result = rag.query(\"What is important for RAG?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bea8b",
   "metadata": {},
   "source": [
    "## 3. Selecting Embedding Models (15 minutes)\n",
    "\n",
    "Factors to consider when choosing an embedding model:\n",
    "1. Task specificity (general vs. domain-specific)\n",
    "2. Model size and computational requirements\n",
    "3. Supported languages\n",
    "4. Fine-tuning capabilities\n",
    "5. Licensing and usage restrictions\n",
    "\n",
    "Popular embedding models:\n",
    "- Sentence-BERT (SBERT)\n",
    "- Universal Sentence Encoder\n",
    "- OpenAI embeddings (e.g., text-embedding-ada-002)\n",
    "- Domain-specific models (e.g., SciBERT for scientific texts)\n",
    "\n",
    "Let's compare two embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_embeddings(model1, model2, sentences):\n",
    "    embeddings1 = model1.encode(sentences)\n",
    "    embeddings2 = model2.encode(sentences)\n",
    "    \n",
    "    similarity1 = cosine_similarity(embeddings1)\n",
    "    similarity2 = cosine_similarity(embeddings2)\n",
    "    \n",
    "    print(f\"Model 1 similarities:\\n{similarity1}\\n\")\n",
    "    print(f\"Model 2 similarities:\\n{similarity2}\")\n",
    "\n",
    "model1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model2 = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on the rug.\",\n",
    "    \"Dogs are loyal pets.\"\n",
    "]\n",
    "\n",
    "compare_embeddings(model1, model2, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e000395",
   "metadata": {},
   "source": [
    "## 4. Implementing Embedding Models in RAG (15 minutes)\n",
    "\n",
    "Let's implement a more advanced RAG system using different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbe5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedRAG:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "\n",
    "    def add_documents(self, docs):\n",
    "        self.documents.extend(docs)\n",
    "        new_embeddings = self.model.encode(docs)\n",
    "        self.embeddings.extend(new_embeddings)\n",
    "\n",
    "    def query(self, query, top_k=3):\n",
    "        query_embedding = self.model.encode(query)\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# Usage\n",
    "rag = AdvancedRAG()\n",
    "\n",
    "documents = [\n",
    "    \"Embedding models convert text to vectors.\",\n",
    "    \"RAG systems use embeddings for efficient retrieval.\",\n",
    "    \"Language models generate human-like text.\",\n",
    "    \"Vectorization is key to many NLP tasks.\"\n",
    "]\n",
    "\n",
    "rag.add_documents(documents)\n",
    "\n",
    "query = \"How are embeddings used in RAG?\"\n",
    "results = rag.query(query)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.4f} - Document: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69cdc6",
   "metadata": {},
   "source": [
    "## 5. Evaluating Embedding Model Performance (10 minutes)\n",
    "\n",
    "Metrics for evaluating embedding models in RAG:\n",
    "1. Retrieval accuracy\n",
    "2. Mean Reciprocal Rank (MRR)\n",
    "3. Normalized Discounted Cumulative Gain (NDCG)\n",
    "4. Semantic similarity correlation\n",
    "\n",
    "Let's implement a simple evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8301041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "def evaluate_embeddings(model, queries, relevant_docs):\n",
    "    rag = AdvancedRAG(model_name=model)\n",
    "    rag.add_documents(relevant_docs)\n",
    "    \n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    for query in queries:\n",
    "        results = rag.query(query, top_k=len(relevant_docs))\n",
    "        true_relevance = [1 if doc in relevant_docs else 0 for doc, _ in results]\n",
    "        scores = [score for _, score in results]\n",
    "        \n",
    "        y_true.append(true_relevance)\n",
    "        y_scores.append(scores)\n",
    "    \n",
    "    map_score = label_ranking_average_precision_score(y_true, y_scores)\n",
    "    return map_score\n",
    "\n",
    "# Example usage\n",
    "queries = [\"What are embeddings?\", \"How does RAG work?\"]\n",
    "relevant_docs = [\n",
    "    \"Embedding models convert text to vectors.\",\n",
    "    \"RAG systems use embeddings for efficient retrieval.\",\n",
    "    \"Embeddings capture semantic meaning of text.\"\n",
    "]\n",
    "\n",
    "model1_score = evaluate_embeddings('all-MiniLM-L6-v2', queries, relevant_docs)\n",
    "model2_score = evaluate_embeddings('paraphrase-multilingual-MiniLM-L12-v2', queries, relevant_docs)\n",
    "\n",
    "print(f\"Model 1 MAP score: {model1_score:.4f}\")\n",
    "print(f\"Model 2 MAP score: {model2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e3b59",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "In this lesson, we've explored the crucial role of embedding models in RAG systems. We've learned about different types of embedding models, how to select and implement them, and how to evaluate their performance in the context of RAG.\n",
    "\n",
    "Are there any questions about embedding models or their application in RAG systems?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. Sentence-Transformers documentation: https://www.sbert.net/\n",
    "2. \"Understanding Embeddings in NLP\" article: https://towardsdatascience.com/t2v-a-comprehensive-guide-to-generating-document-embeddings-eaaf5e5ea58d\n",
    "3. \"Evaluation of Sentence Embeddings in Downstream and Linguistic Probing Tasks\" paper: https://arxiv.org/abs/1806.06259\n",
    "4. Hugging Face Embeddings documentation: https://huggingface.co/docs/transformers/main_classes/embeddings\n",
    "\n",
    "In our next lesson, we'll explore vector databases, which are crucial for storing and retrieving the embeddings we've learned about today."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
