{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4766196a",
   "metadata": {},
   "source": [
    "# Lesson 27: Data Preparation and Preprocessing\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Data Preparation and Preprocessing for RAG systems. In this 60-minute session, we'll explore the crucial steps involved in preparing and processing data for effective retrieval and generation. We'll cover text loading, segmentation, cleaning, and integration with search engines.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Load and parse different types of documents\n",
    "2. Implement text segmentation techniques\n",
    "3. Apply data cleaning and normalization methods\n",
    "4. Set up and use Elasticsearch for text indexing\n",
    "5. Understand and implement various data processing techniques\n",
    "\n",
    "## 1. Text Loading and Parsing (15 minutes)\n",
    "\n",
    "Let's start by implementing functions to load and parse different types of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8326c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def load_pdf_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        return ' '.join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "def load_html_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        return soup.get_text()\n",
    "\n",
    "def load_web_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def load_document(path_or_url):\n",
    "    if path_or_url.startswith('http'):\n",
    "        return load_web_page(path_or_url)\n",
    "    \n",
    "    _, file_extension = os.path.splitext(path_or_url)\n",
    "    if file_extension == '.txt':\n",
    "        return load_text_file(path_or_url)\n",
    "    elif file_extension == '.pdf':\n",
    "        return load_pdf_file(path_or_url)\n",
    "    elif file_extension in ['.html', '.htm']:\n",
    "        return load_html_file(path_or_url)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "# Usage\n",
    "text = load_document('example.pdf')\n",
    "print(text[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69937f17",
   "metadata": {},
   "source": [
    "## 2. Text Segmentation (15 minutes)\n",
    "\n",
    "Next, let's implement text segmentation to break down large documents into manageable chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def segment_text(text, max_chunk_size=1000, overlap=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        if current_size + sentence_size > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = current_chunk[-overlap//10:]  # Keep some sentences for overlap\n",
    "                current_size = sum(len(s) for s in current_chunk)\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Usage\n",
    "document = load_document('example.pdf')\n",
    "segments = segment_text(document)\n",
    "print(f\"Number of segments: {len(segments)}\")\n",
    "print(f\"First segment: {segments[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a14d1",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Normalization (10 minutes)\n",
    "\n",
    "Let's implement some basic data cleaning and normalization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Replace common contractions\n",
    "    contractions = {\n",
    "        \"n't\": \" not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'d\": \" would\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = normalize_text(text)\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "raw_text = \"Here's an example text with some noise!!! It's got 123 numbers and special chars.\"\n",
    "processed_text = preprocess_text(raw_text)\n",
    "print(f\"Original: {raw_text}\")\n",
    "print(f\"Processed: {processed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616b49b",
   "metadata": {},
   "source": [
    "## 4. Setting up Elasticsearch (10 minutes)\n",
    "\n",
    "Now, let's set up Elasticsearch for efficient text indexing and search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e25871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "def create_index(index_name):\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"content\": {\"type\": \"text\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": 384}  # Adjust dims based on your embedding model\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=settings)\n",
    "\n",
    "def index_documents(index_name, documents):\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"content\": doc,\n",
    "                # Add embedding here if you're using vector search\n",
    "            }\n",
    "        }\n",
    "        for doc in documents\n",
    "    ]\n",
    "    bulk(es, actions)\n",
    "\n",
    "# Usage\n",
    "index_name = \"rag_documents\"\n",
    "create_index(index_name)\n",
    "documents = segment_text(load_document('example.pdf'))\n",
    "index_documents(index_name, documents)\n",
    "print(f\"Indexed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e2ef6",
   "metadata": {},
   "source": [
    "## 5. Data Processing Methods and Tools (15 minutes)\n",
    "\n",
    "Let's explore some additional data processing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sorted_items = sorted(zip(tfidf_matrix.tocsr().data, feature_names))\n",
    "    keywords = [item[1] for item in sorted_items[-top_n:]]\n",
    "    return keywords\n",
    "\n",
    "def generate_summary(text, ratio=0.2):\n",
    "    return summarize(text, ratio=ratio)\n",
    "\n",
    "def detect_language(text):\n",
    "    from langdetect import detect\n",
    "    return detect(text)\n",
    "\n",
    "# Usage\n",
    "sample_text = load_document('example.txt')\n",
    "keywords = extract_keywords(sample_text)\n",
    "summary = generate_summary(sample_text)\n",
    "language = detect_language(sample_text)\n",
    "\n",
    "print(f\"Keywords: {keywords}\")\n",
    "print(f\"Summary: {summary[:200]}...\")\n",
    "print(f\"Detected Language: {language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd21157",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "In this lesson, we've covered essential data preparation and preprocessing techniques for RAG systems. We've learned how to load and parse different document types, segment text, clean and normalize data, set up Elasticsearch for indexing, and implement various data processing methods.\n",
    "\n",
    "Are there any questions about the data preparation and preprocessing steps we've covered?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. NLTK documentation: https://www.nltk.org/\n",
    "2. Elasticsearch Python Client: https://elasticsearch-py.readthedocs.io/\n",
    "3. \"Text Preprocessing in Python: Steps, Tools, and Examples\" article: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "4. Gensim library documentation: https://radimrehurek.com/gensim/\n",
    "\n",
    "In our next lesson, we'll focus on building the vector database with Milvus for efficient similarity search in our RAG system."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
