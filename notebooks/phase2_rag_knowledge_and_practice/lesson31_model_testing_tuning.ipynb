{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d198e0f8",
   "metadata": {},
   "source": [
    "# Lesson 31: Model Testing and Performance Tuning\n",
    "\n",
    "## Introduction (2 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Testing and Performance Tuning for RAG systems. In this 30-minute session, we'll explore how to evaluate the performance of our RAG model and implement strategies to improve its accuracy and efficiency.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Implement test cases for evaluating RAG model performance\n",
    "2. Use appropriate metrics for assessing retrieval and generation quality\n",
    "3. Apply techniques to tune and optimize RAG model performance\n",
    "\n",
    "## 1. Implementing Test Cases (10 minutes)\n",
    "\n",
    "Let's start by creating a set of test cases for our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rag_proxy_service import RAGProxyService  # Assume this is our implementation from the previous lesson\n",
    "\n",
    "class RAGTester:\n",
    "    def __init__(self, rag_service):\n",
    "        self.rag_service = rag_service\n",
    "        self.test_cases = []\n",
    "\n",
    "    def load_test_cases(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.test_cases = json.load(f)\n",
    "\n",
    "    def run_tests(self):\n",
    "        results = []\n",
    "        for case in self.test_cases:\n",
    "            query = case['query']\n",
    "            expected_answer = case['expected_answer']\n",
    "            actual_answer = self.rag_service.process_query(query)\n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'expected_answer': expected_answer,\n",
    "                'actual_answer': actual_answer\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "rag_service = RAGProxyService(embedding_model, vector_db, language_model)\n",
    "tester = RAGTester(rag_service)\n",
    "tester.load_test_cases('test_cases.json')\n",
    "test_results = tester.run_tests()\n",
    "\n",
    "# Example test_cases.json structure:\n",
    "# [\n",
    "#     {\n",
    "#         \"query\": \"What is retrieval-augmented generation?\",\n",
    "#         \"expected_answer\": \"Retrieval-augmented generation is a technique that combines information retrieval with text generation to produce more accurate and informed responses.\"\n",
    "#     },\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2fd2e",
   "metadata": {},
   "source": [
    "## 2. Evaluating Model Performance (10 minutes)\n",
    "\n",
    "Now, let's implement some metrics to evaluate our RAG model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.rouge = Rouge()\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def calculate_rouge(self, reference, hypothesis):\n",
    "        scores = self.rouge.get_scores(hypothesis, reference)\n",
    "        return scores[0]['rouge-l']['f']\n",
    "\n",
    "    def calculate_bleu(self, reference, hypothesis):\n",
    "        return sentence_bleu([reference.split()], hypothesis.split())\n",
    "\n",
    "    def calculate_semantic_similarity(self, reference, hypothesis):\n",
    "        ref_embedding = self.embedding_model.encode(reference)\n",
    "        hyp_embedding = self.embedding_model.encode(hypothesis)\n",
    "        return cosine_similarity([ref_embedding], [hyp_embedding])[0][0]\n",
    "\n",
    "    def evaluate(self, test_results):\n",
    "        metrics = {\n",
    "            'rouge': [],\n",
    "            'bleu': [],\n",
    "            'semantic_similarity': []\n",
    "        }\n",
    "        for result in test_results:\n",
    "            metrics['rouge'].append(self.calculate_rouge(result['expected_answer'], result['actual_answer']))\n",
    "            metrics['bleu'].append(self.calculate_bleu(result['expected_answer'], result['actual_answer']))\n",
    "            metrics['semantic_similarity'].append(self.calculate_semantic_similarity(result['expected_answer'], result['actual_answer']))\n",
    "        \n",
    "        return {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "\n",
    "# Usage\n",
    "evaluator = RAGEvaluator(embedding_model)\n",
    "evaluation_results = evaluator.evaluate(test_results)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3e058",
   "metadata": {},
   "source": [
    "## 3. Performance Tuning Strategies (8 minutes)\n",
    "\n",
    "Based on the evaluation results, we can apply several strategies to improve our RAG model's performance:\n",
    "\n",
    "1. Adjust retrieval parameters:\n",
    "   - Modify the number of retrieved documents (top_k)\n",
    "   - Experiment with different similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_retrieval(rag_service, queries, top_k_values=[3, 5, 10]):\n",
    "    results = {}\n",
    "    for top_k in top_k_values:\n",
    "        rag_service.top_k = top_k\n",
    "        responses = [rag_service.process_query(q) for q in queries]\n",
    "        results[top_k] = evaluator.evaluate(responses)\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "tune_results = tune_retrieval(rag_service, [case['query'] for case in test_cases])\n",
    "best_top_k = max(tune_results, key=lambda k: tune_results[k]['rouge'])\n",
    "print(f\"Best top_k value: {best_top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564225ba",
   "metadata": {},
   "source": [
    "2. Refine prompt engineering:\n",
    "   - Experiment with different prompt structures\n",
    "   - Add or modify system messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bce66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_prompt(rag_service, queries, prompt_templates):\n",
    "    results = {}\n",
    "    for template in prompt_templates:\n",
    "        rag_service.prompt_template = template\n",
    "        responses = [rag_service.process_query(q) for q in queries]\n",
    "        results[template] = evaluator.evaluate(responses)\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "prompt_templates = [\n",
    "    \"Context:\\n{context}\\n\\nQuery: {query}\\nAnswer:\",\n",
    "    \"Given the following information:\\n{context}\\n\\nPlease answer: {query}\",\n",
    "    \"Use the context below to answer the question:\\nContext: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "]\n",
    "prompt_results = tune_prompt(rag_service, [case['query'] for case in test_cases], prompt_templates)\n",
    "best_prompt = max(prompt_results, key=lambda k: prompt_results[k]['rouge'])\n",
    "print(f\"Best prompt template: {best_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009ebc9",
   "metadata": {},
   "source": [
    "3. Fine-tune the language model:\n",
    "   - If using a local model, consider fine-tuning on domain-specific data\n",
    "   - For API-based models, experiment with different model versions or parameters\n",
    "\n",
    "## Conclusion and Next Steps (2 minutes)\n",
    "\n",
    "In this lesson, we've explored methods for testing and tuning the performance of our RAG model. We implemented test cases, evaluation metrics, and strategies for improving retrieval and generation quality. Remember that performance tuning is an iterative process, and the best configuration may vary depending on your specific use case and data.\n",
    "\n",
    "In our next lesson, we'll focus on implementing the frontend and backend interfaces for our RAG system, creating a complete end-to-end application.\n",
    "\n",
    "Are there any questions about model testing or performance tuning strategies?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"ROUGE: A Package for Automatic Evaluation of Summaries\" paper: https://www.aclweb.org/anthology/W04-1013/\n",
    "2. NLTK BLEU score documentation: https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score\n",
    "3. \"Fine-tuning Language Models from Human Preferences\" paper: https://arxiv.org/abs/1909.08593\n",
    "4. Hugging Face's \"Trainer\" class for fine-tuning: https://huggingface.co/transformers/main_classes/trainer.html\n",
    "\n",
    "For the next lesson, please review the concepts of web development and API design, as we'll be implementing the frontend and backend interfaces for our RAG system."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
